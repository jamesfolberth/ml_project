\documentclass{article}
%\documentclass[draft]{article}
% Functions, packages, etc.
%[[[
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}

\usepackage{graphicx}
%\usepackage{subfig}
\usepackage[labelfont=bf]{caption}
%\usepackage[labelfont=bf]{subcaption}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\pagenumbering{arabic}
\usepackage{hyperref}
\usepackage{enumerate}
\usepackage{color}
%\numberwithin{equation}{section}
%\usepackage{soul} % for \ul - a ``better'' underlining command

%\usepackage{colortbl} % for coloring \multicolumn (tabular in general, I think)
% For \rowcolor
%\definecolor{table_header}{gray}{0.5}
%\definecolor{table_data}{gray}{0.85}


%% Inserting code and syntax highlighting
%% [[[
%\usepackage{listings} % like verbatim, but allows for syntax highlighting and more
%\usepackage{color} % colors
%\usepackage[usenames,dvipsnames]{xcolor}% More colors
%\usepackage{caption} % captions
%\DeclareCaptionFont{white}{\color{white}}
%\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
%\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
%\usepackage{framed} % put a frame around things
%
%% define some custom colors
%\definecolor{dkgreen}{rgb}{0,0.6,0}
%\definecolor{lgreen}{rgb}{0.25,1,0}
%\definecolor{purple}{rgb}{0.35,0.02,0.48}
%
%% Some changes to MATLAB/GNU Octave language in listings
%\lstset{frame=tbrl,
%    language=Matlab,
%    aboveskip=3mm,
%    belowskip=3mm,
%    belowcaptionskip=3mm,
%    showstringspaces=false,
%    columns=flexible,
%    basicstyle={\small\ttfamily\color{black}},
%    numbers=left,
%    numberstyle=\tiny\color{purple},
%    keywordstyle=\color{dkgreen},
%    commentstyle=\color{red},
%    stringstyle=\color{purple},
%    breaklines=true,
%    breakatwhitespace=true,
%    tabsize=4,
%    rulecolor=\color{black},
%    morekeywords={string,fstream}
%}
%% ]]]


%My Functions
\newcommand{\diff}[2]{\dfrac{d #1}{d #2}}
\newcommand{\diffn}[3]{\dfrac{d^{#3} #1}{d {#2}^{#3}}}
\newcommand{\pdiff}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\pdiffn}[3]{\dfrac{\partial^{#3} #1}{\partial {#2}^{#3}}}
\newcommand{\drm}{\mathrm{d}}
\newcommand{\problemline}{\rule{\textwidth}{0.25mm}}
\newcommand{\problem}[1]{\problemline\\#1\\\problemline\vspace{10pt}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\qline}[2]{\qbezier(#1)(#1)(#2)}
\newcommand{\abox}[1]{\begin{center}\fbox{#1}\end{center}}
\newcommand{\lie}{\mathcal{L}}
\newcommand{\defeq}{\stackrel{\operatorname{def}}{=}}


% AMS theorem stuff
% [[[
\newtheoremstyle{mystuff}{}{}{\itshape}{}{\bfseries}{:}{.5em}{}
\theoremstyle{mystuff}
\newtheorem{definition}{Definition}[section]
\newtheorem*{definition*}{Definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{lemma*}{Lemma}
\newtheorem*{proposition*}{Proposition}
\newtheorem{corallary}{Corallary}
\newtheorem*{remark}{Remark}

\newtheoremstyle{myexample}{}{}{}{}{\bfseries}{:}{.5em}{}
\theoremstyle{myexample}
\newtheorem*{example*}{Example}


% Stolen from http://tex.stackexchange.com/questions/8089/changing-style-of-proof
\makeatletter \renewenvironment{proof}[1][\proofname] {\par\pushQED{\qed}\itshape\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{:}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother

% Stolen from http://tex.stackexchange.com/questions/12913/customizing-theorem-name
\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{:}{.5em}{\thmnote{#3's }#1}
\theoremstyle{named}
\newtheorem*{namedtheorem}{Theorem}
% ]]]

%]]]

% Output Control Variables
\def\true{1}
\def\false{0}
\def\figures{1}
\def\tables{1}

\setcounter{subsection}{-1}

\title{Final Project - Writeup}
\date{16 December, 2015}
\author{Andrew Cowley (cowleya), James Folberth, Derek Reiersen, Ben Wiley}

\begin{document}
\maketitle

We competed in the science question answering project.  Our team name is Milksteak, after the fine cuisine. The science question answer project is posed as follows: 

    \begin{center}
        \textit{We are given a training and testing set of questions and answers, and each question has four possible answers, only one of which is correct.}
    \end{center}

Questions generally include a few proper nouns, and each answer corresponds, almost directly, to a Wikipedia page title.  Our code should attempt to answer the questions correctly. \\

We used the Python \texttt{wikipedia} module to download text data from Wikipedia pages.  Our code is available on James' \href{https://github.com/jamesfolberth/ml_project}{Github page}.\\

\subsection{NLP and Feature Vectors}
\label{ssec:nlp}
One of our first ideas is to lemmatize, and make $n$-grams out of, the text in the questions and the Wiki page summary and content.  We also attempted to emphasize proper nouns in the question and Wiki summaries.  We vectorized the lemmatized ``feature strings'' using \texttt{sklearn}'s TF-IDF vectorizer.  It is common to use the cosine similarity measure to compare these feature vectors as a measure of the similarity of the two documents.

\[ \text{similarity}_{ij} = \cos(\theta_{ij}) = \dfrac{\langle v_i, v_j\rangle}{\|v_i\|\|v_j\|}. \] 

\noindent The higher the inner product between two feature vectors $v_i$ and $v_j$, the more similar the two documents should be.  We ended up computing the similarity of the question and each of the four corresponding questions.  The answer with the highest similarity will hopefully be the right answer.  Using this method only, we scored $70\%$ in our ``cross-validation'' and also on Kaggle.  James took the lead on this.\\

%-----------------------------------------------------------------------

\subsection{Using NLP to Eliminate Answers}
We began this project by inspecting a sample of questions and answers to try to identify a structure that we exploit in order to determine correct answers. When we observed that a majority of the questions were phrased in a similar manner to the examples shown below, we decided that an NLP approach to eliminate answers could be effective:

\begin{enumerate}
      \item Antibodies that localize to {\color{red}this} {\color{green}organelle} are associated with Sjogren\'s ("SHOW-grenz") syndrome, and a deficiency of a phosphatase localized to it causes Lowe\'s syndrome.

	(a) {\color{green}Golgi Apparatus} (b) Apoptosis (c) {\color{green}Peroxisome} (d) Collagen

      \item {\color{red}This} {\color{green}phylum's} members have a pair of pouches that split from the archenteron, with each pouch constricting into three portions. 

	(a) Xylem (b) {\color{green}Echinoderm} (c) Sponge (d) Mitochondrian
\end{enumerate} 

These questions, which make up roughly 63$\%$ of the dataset, use the keywords "this" and "these" to indicate a general category that an answer must fit. For example 1 above the correct answer must be an organelle which immediately eliminates answers (b) and (d), while in example 2 only answer (b) fits the category of phylum and must therefor be the correct answer. To analyze these questions we first utilized the NLTK to  perform part-of-speech tagging on all of the questions in the dataset. These POS tags were used to find the noun that most immediately follows the keywords, which became the answer category. We then attempted to utilize the PyGoogle module to make queries to Google to help determine whether an answer was a member of the necessary category for each question. The Google queries continually threw exceptions and we eventually decided to use only the Wikipedia data to eliminate answers. To do this we counted the number of occurrences of the category word in each answer's Wiki page. To reduce the false positive rate we only answered questions where the category appeared in a single answer's Wiki at least "max num" times and appeared in every other answers' Wiki less than "min num" times. The performance for different combinations  of these parameters are summarized in the table below:\\

\begin{center}
\begin{tabular}{l*{6}{c}r}
Max Num             & None & 1 & 2 & 1 & 1  & 2 & 3 \\
Min Num             & None & None & None & 1 & 2  & 2 & 2 \\
\hline
$\%$ Questions Answered & $\%$63 & $\%$56 & $\%$51 & $\%$11 & $\%$23 & $\%$19 & $\%$16  \\
$\%$ Correct Answers  & $\%$55 & $\%$57 & $\%$56 & $\%$85 &  $\%$77 & $\%$79 &  $\%$84  \\
\end{tabular}
\end{center}

We tried integrating some of these answers with the ensemble learner, but were unable to boost performance. We can still conclude from these results that only a single word in the questions can often be used to eliminate multiple answer options, and utilizing additional web resources would very likely make this method more effective.

%----------------------------------------------------------------------

\subsection{Topic Model}
\label{ssec:topic_model}
It is also natural to attempt to categorize each question and answer as physics, chemistry, math, etc..  We elected to use LDA to sort this out.  After training the topic model on the feature vectors from section \ref{ssec:nlp}, we compute the topic probabilities for each question and answer.  We compare these probability vectors using the cosine similarity measure.  James and Ben took the lead on this.\\

In selecting the topics to construct the term-document matrix, we chose to utilize the \texttt{Categories} section from the Wikipedia pages; that is, for each answer choice $A$ through $D$, each category to the corresponding Wikipedia page is used as a topic. For example, the answer \texttt{Duino Elegies} includes the following categories, 

\[
	\begin{array}{lll}
		\text{Topic} & & \text{Documents} \\ \hline
		\texttt{Duino Elegies} & & \text{1912 poems}, \, \text{1922 poems}, \text{1923 books}, \\
		& & \text{Articles containing German-language text}, \\
		& &  \text{Articles containing non-English-language text}, \\
		& &  \text{Articles that link to foreign-language Wikisources}, \, \ldots
	\end{array}
\]

This process initially included many categories that were too vague or general for our term-document matrix. Considering the table above, the document ``Articles that link to foreign-language Wikisources'' is not specific enough to its corresponding topic, \texttt{Duino Elegies}. Accordingly, words corresponding to weak documents were identified, and sentences containing these words were subsequently removed from the categories. For \texttt{Duino Elegies}, this resulted more concise documents,

\[
	\begin{array}{lll}
		\text{Topic} & & \text{Documents} \\ \hline
		\texttt{Duino Elegies} & & \text{Austrian books}, \, \text{1912 poems}, \, \text{1922 poems}, \\
		& & \text{1923 books}, \, \text{Poetry collections}, \\
		& & \text{Poetry by Rainer Maria Rilke}
	\end{array}
\]

\noindent Streamlining this process in Python reduced excess noise associated with our LDA implementation.


\subsection{Logistic Regression}
Logistic regression was used to attempt to answer the science questions.  L2 Regularization was used.  Many different feature sets where used to train the lr model.  The following subsections will go into detail each feature set used, and the results from said feature set. \\

The primary regression parameters correspond to the NLP Similarity and Topic Model approaches our team developed for this project. Logistic regression internally optimizes the weights associated with each task, favoring a combination of the main algorithms with higher preference given to the NLP process. Beta coefficients for each task are displayed below,

\[
	\begin{array}{ccc}
		\textbf{$\beta$}: & \text{NLP Similarity} & \text{Topic Model} \\\hline
			& 1.160 & 1.020
	\end{array}
\]

Additional regression parameters include lengths associated with various data from the Wikipedia pages. In particular, the corresponding Wikipedia page to an answer choice contains various information on that topic,

\[
	\begin{array}{lllllll}
		\text{Wiki page}: & [\texttt{links}, & \texttt{title}, & \texttt{summary}, & \texttt{content}, & \texttt{sections}, & \texttt{categories}]
	\end{array}
\]

The associated lengths with respect to each of these categories was included in our final regression model, providing weak but substantial improvement to our prediction accuracy. The $\beta$ coefficients  to each data category are provided below,

\[
	\begin{array}{lcccccc}
		\textbf{$\beta$}: & \texttt{links} & \texttt{title} & \texttt{summary} & \texttt{content} & \texttt{sections} & \texttt{categories} \\\hline
			& -0.1357 & -1.1363 & 0.2546 & -0.5537 & 0 & 0.0122
	\end{array}
\]

\noindent As an interpretation to these parameters, a longer Wikipedia summary in relation to an answer choice indicates a slightly higher possibility for that answer choice to be correct, while a longer Wikipedia title, or the length of the answer choice, indicates a strong possibility that the answer choice is incorrect. Both high NLP Similarity and Topic Model scores indicate a strong preference that a corresponding answer choice is correct.

\subsubsection{Baseline: Sample Questions Only}
This was the first featureset we used, and was used as our baseline percentage.  The training data consisted of 5441 questions.  Each question was a sentence, that had four possible answers.  The label assigned to each question was the correct answer, which is given in the training data set.  The sentences were put through a count vectorizer, using a list of common English stop words and removing the accents from each letter.  The testing set follows the same format, with 785 test questions.  The difference is that the test questions do not have a known correct answer, but four possible answers. \\

There was a slight problem, the testing set had questions that were not covered by correct answers in the training sets.  While additional features could be added to cover these missing labels, a workaround was used for the baseline.  Once the LR was trained, the probabilities for each possible question was generated.  Then, the probabilities from logistic regression for the four possible answers were calculated.  If all answers were assigned a probability, meaning they were in the training set, the answer with the highest probability was chosen.  If not, and there is a answer with a zero probability, the difference between the first best and second best answer was examined.  If the difference between their probabilities was small, then the unknown question was chosen, because the classifier cannot decide between two of the known options. \\

This method scored surprisingly well, scoring a 63 percent on Kaggle.  This came as a shock, because the training data is not complete for the testing data.  Cross validation was not performed with this method( but was performed for all of the remaining logistic regression attempts) because there was simply not enough information in the training set, cross validation could remove more valid answers from all of the labels.  The success of this method showed that there is promise in cross-validation, and that better percentages by adding further features could possibly be obtained.

\subsubsection{Wikipedia summary sentences}
Since the previous logistic regression with limited data gave good results, we tried to expand on it using Wikipedia data.  The Wikipedia data we extracted for each question has a summary section, which contains a small amount of sentences that give a good summary on each answer.\\

Using the whole summary did not work very well.  However, looking at the structure of the testing data, each test question is approximately one sentence, while each training example is also approximately one sentence.  To emulate this, each Wikipedia summary was separated by periods.  This gave multiple strings that most likely were a single sentence.  This was then added to the logistic regression model, as training data.\\

In this case, there was enough data to use cross validation.  We used 10 percent of the training set as the testing set.  After separating the training into two sets of training and test data, the cross-validation achieved a 72 percent accuracy.  When submitted to Kaggle, it scored a 65 percent accuracy.\\

Other wikipedia features were tried, such as using the categories, but no noticeable increase in accuracy was found.\\
\label{ssec:log_reg}

\subsection{Final ensemble learner}
Because both logistic regression, topic models, and cosine similarity gave good results, we attempted to combine the results into an ensemble learner.  To accomplish this, the logistic regression moe\\l with Wikipedia summaries found the probability of the four possible answers, and normalized them so they would add up to 100 percent.  The topic model and cosine similarity gave similar measurements, and were normalized too add up to one as well, for the four possible answers.  A scaling factor was given to each of these vectors, and they were summed.  The answer with the highest sum was chosen.  The cosine similarity was given a scaling factor of 1.5, the topic model a scaling factor of .75, and the logistic regression learner a scaling factor of one.

This achieved an average of 82.5 percent in cross validation.  In addition to this, the final hidden testing set got a 79 percent accuracy on Kaggle.  This was the highest percentage we achieved. \\



References:\\
\problemline
\begin{itemize}
   \item \url{https://pypi.python.org/pypi/wikipedia/}
   \item \url{https://github.com/jamesfolberth/ml_project}
\end{itemize}

% References
%\clearpage
%\bibliographystyle{siam}
%\bibliography{LaTeX_article}

\end{document}

% vim: set spell:
% vim: foldmarker=[[[,]]]
