\documentclass{article}
%\documentclass[draft]{article}
% Functions, packages, etc.
%[[[
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}

\usepackage{graphicx}
%\usepackage{subfig}
\usepackage[labelfont=bf]{caption}
%\usepackage[labelfont=bf]{subcaption}
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\pagenumbering{arabic}
\usepackage{hyperref}
\usepackage{enumerate}
%\numberwithin{equation}{section}
%\usepackage{soul} % for \ul - a ``better'' underlining command

%\usepackage{colortbl} % for coloring \multicolumn (tabular in general, I think)
% For \rowcolor
%\definecolor{table_header}{gray}{0.5}
%\definecolor{table_data}{gray}{0.85}


%% Inserting code and syntax highlighting
%% [[[
%\usepackage{listings} % like verbatim, but allows for syntax highlighting and more
%\usepackage{color} % colors
%\usepackage[usenames,dvipsnames]{xcolor}% More colors
%\usepackage{caption} % captions
%\DeclareCaptionFont{white}{\color{white}}
%\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
%\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
%\usepackage{framed} % put a frame around things
%
%% define some custom colors
%\definecolor{dkgreen}{rgb}{0,0.6,0}
%\definecolor{lgreen}{rgb}{0.25,1,0}
%\definecolor{purple}{rgb}{0.35,0.02,0.48}
%
%% Some changes to MATLAB/GNU Octave language in listings
%\lstset{frame=tbrl,
%    language=Matlab,
%    aboveskip=3mm,
%    belowskip=3mm,
%    belowcaptionskip=3mm,
%    showstringspaces=false,
%    columns=flexible,
%    basicstyle={\small\ttfamily\color{black}},
%    numbers=left,
%    numberstyle=\tiny\color{purple},
%    keywordstyle=\color{dkgreen},
%    commentstyle=\color{red},
%    stringstyle=\color{purple},
%    breaklines=true,
%    breakatwhitespace=true,
%    tabsize=4,
%    rulecolor=\color{black},
%    morekeywords={string,fstream}
%}
%% ]]]


%My Functions
\newcommand{\diff}[2]{\dfrac{d #1}{d #2}}
\newcommand{\diffn}[3]{\dfrac{d^{#3} #1}{d {#2}^{#3}}}
\newcommand{\pdiff}[2]{\dfrac{\partial #1}{\partial #2}}
\newcommand{\pdiffn}[3]{\dfrac{\partial^{#3} #1}{\partial {#2}^{#3}}}
\newcommand{\drm}{\mathrm{d}}
\newcommand{\problemline}{\rule{\textwidth}{0.25mm}}
\newcommand{\problem}[1]{\problemline\\#1\\\problemline\vspace{10pt}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\qline}[2]{\qbezier(#1)(#1)(#2)}
\newcommand{\abox}[1]{\begin{center}\fbox{#1}\end{center}}
\newcommand{\lie}{\mathcal{L}}
\newcommand{\defeq}{\stackrel{\operatorname{def}}{=}}


% AMS theorem stuff
% [[[
\newtheoremstyle{mystuff}{}{}{\itshape}{}{\bfseries}{:}{.5em}{}
\theoremstyle{mystuff}
\newtheorem{definition}{Definition}[section]
\newtheorem*{definition*}{Definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem*{lemma*}{Lemma}
\newtheorem*{proposition*}{Proposition}
\newtheorem{corallary}{Corallary}
\newtheorem*{remark}{Remark}

\newtheoremstyle{myexample}{}{}{}{}{\bfseries}{:}{.5em}{}
\theoremstyle{myexample}
\newtheorem*{example*}{Example}


% Stolen from http://tex.stackexchange.com/questions/8089/changing-style-of-proof
\makeatletter \renewenvironment{proof}[1][\proofname] {\par\pushQED{\qed}\itshape\topsep6\p@\@plus6\p@\relax\trivlist\item[\hskip\labelsep\bfseries#1\@addpunct{:}]\ignorespaces}{\popQED\endtrivlist\@endpefalse} \makeatother

% Stolen from http://tex.stackexchange.com/questions/12913/customizing-theorem-name
\newtheoremstyle{named}{}{}{\itshape}{}{\bfseries}{:}{.5em}{\thmnote{#3's }#1}
\theoremstyle{named}
\newtheorem*{namedtheorem}{Theorem}
% ]]]

%]]]

% Output Control Variables
\def\true{1}
\def\false{0}
\def\figures{1}
\def\tables{1}

\setcounter{subsection}{-1}

\title{Final Project - Writeup}
\date{16 December, 2015}
\author{Andrew Cowley, James Folberth, Derek Reiersen, Ben Wiley}

\begin{document}
\maketitle

We competed in the science question answering project.  Our team name is Milksteak, after the fine cuisine.\\

The science question answer project is posed as follows.  We are given a training and testing set of questions and answers.  Each question, generally including a few proper nouns, has four possible answers, only one of which is correct.  Each answer corresponds (almost directly) to a Wikipedia page title.  Our code should attempt to answer the questions correctly.\\

We used the Python \texttt{wikipedia} module to download text data from Wikipedia pages.  Our code is available on James' \href{https://github.com/jamesfolberth/ml_project}{Github page}.\\

\subsection{NLP and Feature Vectors}
\label{ssec:nlp}
One of our first ideas is to lemmatize, and make $n$-grams out of, the text in the questions and the Wiki page summary and content.  We also attempted to emphasize proper nouns in the question and Wiki summaries.  We vectorized the lemmatized ``feature strings'' using \texttt{sklearn}'s TF-IDF vectorizer.  It is common to use the cosine similarity measure to compare these feature vectors as a measure of the similarity of the two documents.

\[ \text{similarity}_{ij} = \cos(\theta_{ij}) = \dfrac{\langle v_i, v_j\rangle}{\|v_i\|\|v_j\|}. \] 

\noindent The higher the inner product between two feature vectors $v_i$ and $v_j$, the more similar the two documents should be.  We ended up computing the similarity of the question and each of the four corresponding questions.  The answer with the highest similarity will hopefully be the right answer.  Using this method only, we scored $70\%$ in our ``cross-validation'' and also on Kaggle.  James took the lead on this.\\


\subsection{Topic Model}
\label{ssec:topic_model}
It is also natural to attempt to categorize each question and answer as physics, chemistry, math, etc..  We elected to use LDA to sort this out.  After training the topic model on the feature vectors from section \ref{ssec:nlp}, we compute the topic probabilities for each question and answer.  We compare these probability vectors using the cosine similarity measure.  James and Ben took the lead on this.\\


\subsection{Logistic Regression}
Logistic regression was used to attempt to answer the science questions.  L2 Regularization was used.  Many different feature sets where used to train the lr model.  The following subsections will go into detail each feature set used, and the results from said featureset
\subsubsection{Baseline: Sample Questions Only}
This was the first featureset we used, and was used as our baseline percentage.  The training data consisted of 5441 questions.  Each question was a sentence, that had four possible answers.  The label assigned to each question was the correct answer, which is given in the training data set.  The sentences were put through a count vectorizer, using a list of common english stop words and removing the accents from each letter.  The testing set follows the same format, with 785 test questions.  The difference is that the test questions do not have a known correct answer, but four possible answers.

There was a slight problem, the testing set had questions that were not covered by correct answers in the training sets.  While additional features could be added to cover these missing labels, a workaround was used for the baseline.  Once the LR was trained, the probabilities for each possible question was generated.  Then, the probabilities from logistic regression for the four possible answers were calculated.  If all answers were assigned a probability, meaning they were in the training set, the answer with the highest probability was chosen.  If not, and there is a answer with a zero probability, the difference between the first best and second best answer was examined.  If the difference between their probabilities was small, then the unknown question was chosen, because the classifier cannot decide between two of the known options.

This method scored surprisingly well, scoring a 63 percent on kaggle.  This came as a shock, because the training data is not complete for the testing data.  Cross validation was not performed with this method( but was performed for all of the remaining logistic regression attempts) because there was simply not enough information in the training set, cross validation could remove more valid answers from all of the labels.  The success of this method showed that there is promise in cross-validation, and that better percentages by adding further features could possibly be obtained.

\subsubsection{Wikipedia summary sentences}




\label{ssec:log_reg}



References:\\
\problemline
\begin{itemize}
   \item \url{https://pypi.python.org/pypi/wikipedia/}
   \item \url{https://github.com/jamesfolberth/ml_project}
\end{itemize}

% References
%\clearpage
%\bibliographystyle{siam}
%\bibliography{LaTeX_article}

\end{document}

% vim: set spell:
% vim: foldmarker=[[[,]]]
