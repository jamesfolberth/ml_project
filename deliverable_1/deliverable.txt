Team: Milksteak

We first implemented code to cache the summary, full text, and categories for all the Wikipedia pages.  We then build a parser that used some NLP tricks to process each question string and the text of the corresponding Wikipedia page.  The parsed/tokenized strings were then used to construct feature vectors.  To produce an answer to a question, we computed the cosine similarity between the question vector and each answer vector (Wikipedia content vector).  Doing just this results in 70% accuracy on our held-out test set (subset of given training set) and also the public test set on Kaggle.

TODO
Combining our methods should produce a better classifier.  We'll have some hyperparameters to tune via cross validation.

